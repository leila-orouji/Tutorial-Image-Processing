{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a keras model\n",
    "# Model building steps: \n",
    "#     -Specify Architecture \n",
    "#     -Compile \n",
    "#     -Fit \n",
    "#     -Predict\n",
    "\n",
    "# Model specification \n",
    "import numpy as np \n",
    "from keras.layers import Dense \n",
    "from keras.models import Sequential \n",
    "\n",
    "predictors = np.loadtxt('cars_scikit_label_dataset.csv', delimiter=',')   #, delimiter=','\n",
    "\n",
    "y= predictors[:,6]\n",
    "x= predictors[:,0:6]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train , y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leila\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 809 samples, validate on 348 samples\n",
      "Epoch 1/14\n",
      "809/809 [==============================] - 1s 932us/step - loss: 1.4448 - acc: 0.3412 - val_loss: 0.8072 - val_acc: 0.5115\n",
      "Epoch 2/14\n",
      "809/809 [==============================] - 0s 73us/step - loss: 0.7299 - acc: 0.4635 - val_loss: 0.7525 - val_acc: 0.4885\n",
      "Epoch 3/14\n",
      "809/809 [==============================] - 0s 73us/step - loss: 0.6792 - acc: 0.5142 - val_loss: 0.7327 - val_acc: 0.5115\n",
      "Epoch 4/14\n",
      "809/809 [==============================] - 0s 97us/step - loss: 0.6670 - acc: 0.4981 - val_loss: 0.7292 - val_acc: 0.5603\n",
      "Epoch 5/14\n",
      "809/809 [==============================] - 0s 80us/step - loss: 0.6513 - acc: 0.5476 - val_loss: 0.7010 - val_acc: 0.5402\n",
      "Epoch 6/14\n",
      "809/809 [==============================] - 0s 111us/step - loss: 0.6166 - acc: 0.5538 - val_loss: 0.6791 - val_acc: 0.5057\n",
      "Epoch 7/14\n",
      "809/809 [==============================] - 0s 83us/step - loss: 0.6000 - acc: 0.5426 - val_loss: 0.6630 - val_acc: 0.5345\n",
      "Epoch 8/14\n",
      "809/809 [==============================] - 0s 76us/step - loss: 0.5858 - acc: 0.5562 - val_loss: 0.6439 - val_acc: 0.5345\n",
      "Epoch 9/14\n",
      "809/809 [==============================] - 0s 88us/step - loss: 0.5564 - acc: 0.5822 - val_loss: 0.6364 - val_acc: 0.5431\n",
      "Epoch 10/14\n",
      "809/809 [==============================] - 0s 75us/step - loss: 0.5416 - acc: 0.5859 - val_loss: 0.6165 - val_acc: 0.5948\n",
      "Epoch 11/14\n",
      "809/809 [==============================] - 0s 77us/step - loss: 0.5287 - acc: 0.6020 - val_loss: 0.6165 - val_acc: 0.5460\n",
      "Epoch 12/14\n",
      "809/809 [==============================] - 0s 87us/step - loss: 0.5157 - acc: 0.5970 - val_loss: 0.5910 - val_acc: 0.5517\n",
      "Epoch 13/14\n",
      "809/809 [==============================] - 0s 84us/step - loss: 0.5053 - acc: 0.5785 - val_loss: 0.5965 - val_acc: 0.6293\n",
      "Epoch 14/14\n",
      "809/809 [==============================] - 0s 76us/step - loss: 0.4827 - acc: 0.6316 - val_loss: 0.6058 - val_acc: 0.6149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2034e96a848>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# fitting a model : \n",
    "#         - Applying backpropagation and gradient descent with your data to update the weights \n",
    "#         - Scaling data before Ã—tting can ease optimization\n",
    "\n",
    "#Fitting \n",
    "from keras.callbacks import EarlyStopping \n",
    "early_stopping_monitor = EarlyStopping(patience=2) \n",
    "\n",
    "# Compiling a model \n",
    "n_cols = x_train.shape[1] \n",
    "model = Sequential() \n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,))) \n",
    "model.add(Dense(100, activation='relu')) \n",
    "model.add(Dense(1)) \n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy']) \n",
    "model.fit(x_train, y_train, validation_split=0.3, nb_epoch=14, callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 100)               700       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,901\n",
      "Trainable params: 10,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[0.569138227640643, 0.6357267939480716]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions =model.predict(x_test) \n",
    "predictions.shape\n",
    "print(predictions.shape[1])\n",
    "probability_true = predictions[:,0]\n",
    "\n",
    "# # Verifying model structure\n",
    "model.summary()\n",
    "\n",
    "print(model.evaluate(x_test, y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_24 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c5079d44bcb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_new_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0madam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Vanishing gradients : Occurs when many layers have very small slopes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_24 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\n",
    "# from keras.optimizers import SGD\n",
    "\n",
    "# # Understanding model optimization\n",
    "\n",
    "# # Stochastic gradient descent \n",
    "# def get_new_model(input_shape ): \n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "#     model.add(Dense(100, activation='relu')) \n",
    "#     model.add(Dense(2, activation='softmax')) \n",
    "#     return(model) \n",
    "\n",
    "\n",
    "# opt = adam(lr=0.001, decay=1e-6)\n",
    "# lr_to_test = [0.001,0.01,1] \n",
    "\n",
    "# # loop over learning rates \n",
    "# for lr in lr_to_test:\n",
    "#     model = get_new_model((n_cols,)) \n",
    "#     my_optimizer = SGD(learning_rate=lr)\n",
    "#     model.compile(optimizer = my_optimizer  , loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.fit(x_train, y_train)\n",
    "    \n",
    "# Vanishing gradients : Occurs when many layers have very small slopes \n",
    "#                     (e.g. due to being on Ã˜at part oftanh curve) \n",
    "#                     In deep networks, updates to backprop were close to 0\n",
    "\n",
    "\n",
    "\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-2,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.9)\n",
    "# optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 7\n",
      "Trainable params: 7\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1041 samples, validate on 116 samples\n",
      "Epoch 1/14\n",
      "1041/1041 [==============================] - 1s 1ms/step - loss: 1.0701 - val_loss: 1.1700\n",
      "Epoch 2/14\n",
      "1041/1041 [==============================] - 0s 17us/step - loss: 1.0459 - val_loss: 1.1358\n",
      "Epoch 3/14\n",
      "1041/1041 [==============================] - 0s 16us/step - loss: 1.0261 - val_loss: 1.1064\n",
      "Epoch 4/14\n",
      "1041/1041 [==============================] - 0s 21us/step - loss: 1.0100 - val_loss: 1.0815\n",
      "Epoch 5/14\n",
      "1041/1041 [==============================] - 0s 17us/step - loss: 0.9972 - val_loss: 1.0607\n",
      "Epoch 6/14\n",
      "1041/1041 [==============================] - 0s 17us/step - loss: 0.9867 - val_loss: 1.0461\n",
      "Epoch 7/14\n",
      "1041/1041 [==============================] - 0s 17us/step - loss: 0.9779 - val_loss: 1.0341\n",
      "Epoch 8/14\n",
      "1041/1041 [==============================] - 0s 30us/step - loss: 0.9694 - val_loss: 1.0232\n",
      "Epoch 9/14\n",
      "1041/1041 [==============================] - 0s 18us/step - loss: 0.9616 - val_loss: 1.0141\n",
      "Epoch 10/14\n",
      "1041/1041 [==============================] - 0s 19us/step - loss: 0.9538 - val_loss: 1.0059\n",
      "Epoch 11/14\n",
      "1041/1041 [==============================] - 0s 17us/step - loss: 0.9464 - val_loss: 0.9988\n",
      "Epoch 12/14\n",
      "1041/1041 [==============================] - 0s 17us/step - loss: 0.9388 - val_loss: 0.9899\n",
      "Epoch 13/14\n",
      "1041/1041 [==============================] - 0s 18us/step - loss: 0.9320 - val_loss: 0.9821\n",
      "Epoch 14/14\n",
      "1041/1041 [==============================] - 0s 19us/step - loss: 0.9250 - val_loss: 0.9746\n",
      "Evaloate:   1.0000543399157749\n"
     ]
    }
   ],
   "source": [
    "# Load layers\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Create an input layer\n",
    "input_tensor = Input(shape=(n_cols,))\n",
    "\n",
    "# Create a dense layer and connect the dense layer to the input_tensor in one step\n",
    "output_tensor = Dense(1)(input_tensor)\n",
    "\n",
    "# Build the model\n",
    "from keras.models import Model\n",
    "model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Import the plotting function\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "# # Plot the model\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "# # Display the image\n",
    "# data = plt.imread('model.png')\n",
    "# plt.imshow(data)\n",
    "# plt.show()\n",
    "\n",
    "# Now fit the model\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=14,\n",
    "          batch_size=128,\n",
    "          validation_split=0.1,\n",
    "          verbose=True)\n",
    "\n",
    "# model.fit (inputFeatures, outputFeatures, batch_size(sets how many sets of data are used for each step of stocastic, gardient descend)\n",
    "#           validation_split, verbos=True: keras prints the log during training which is useful for debuggung \n",
    "#            =False once I know how the model works)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print('Evaloate:  ',model.evaluate(x_test, y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1041 samples, validate on 116 samples\n",
      "Epoch 1/14\n",
      "1041/1041 [==============================] - 1s 1ms/step - loss: 0.8174 - val_loss: 0.8702\n",
      "Epoch 2/14\n",
      "1041/1041 [==============================] - 0s 19us/step - loss: 0.8114 - val_loss: 0.8640\n",
      "Epoch 3/14\n",
      "1041/1041 [==============================] - 0s 19us/step - loss: 0.8056 - val_loss: 0.8588\n",
      "Epoch 4/14\n",
      "1041/1041 [==============================] - 0s 15us/step - loss: 0.8002 - val_loss: 0.8530\n",
      "Epoch 5/14\n",
      "1041/1041 [==============================] - 0s 15us/step - loss: 0.7945 - val_loss: 0.8476\n",
      "Epoch 6/14\n",
      "1041/1041 [==============================] - 0s 26us/step - loss: 0.7891 - val_loss: 0.8417\n",
      "Epoch 7/14\n",
      "1041/1041 [==============================] - 0s 19us/step - loss: 0.7837 - val_loss: 0.8352\n",
      "Epoch 8/14\n",
      "1041/1041 [==============================] - 0s 21us/step - loss: 0.7783 - val_loss: 0.8298\n",
      "Epoch 9/14\n",
      "1041/1041 [==============================] - 0s 20us/step - loss: 0.7735 - val_loss: 0.8252\n",
      "Epoch 10/14\n",
      "1041/1041 [==============================] - 0s 18us/step - loss: 0.7684 - val_loss: 0.8191\n",
      "Epoch 11/14\n",
      "1041/1041 [==============================] - 0s 18us/step - loss: 0.7647 - val_loss: 0.8131\n",
      "Epoch 12/14\n",
      "1041/1041 [==============================] - 0s 19us/step - loss: 0.7606 - val_loss: 0.8105\n",
      "Epoch 13/14\n",
      "1041/1041 [==============================] - 0s 37us/step - loss: 0.7572 - val_loss: 0.8076\n",
      "Epoch 14/14\n",
      "1041/1041 [==============================] - 0s 27us/step - loss: 0.7539 - val_loss: 0.8028\n",
      "Evaloate:   0.8066929276060515\n",
      "[array([[0.151147  ],\n",
      "       [0.30427712],\n",
      "       [0.00597291],\n",
      "       [0.32468617],\n",
      "       [0.00798985],\n",
      "       [0.5209646 ]], dtype=float32), array([0.0512672], dtype=float32)]\n",
      "1.2448861999423797\n"
     ]
    }
   ],
   "source": [
    "# #  Category embeddings are advanced type of layer only available in deeplearning libraries\n",
    "# they are extremely useful for dealing with high cardinality categorical data\n",
    "# embedding layers are usefull for text data such as word to vectors  - map the integers to floats\n",
    "#  flatten layer(output layer)for transform data from multiple dimention to 2D- are useful for dealing\n",
    "#  with time series data,text data and images\n",
    "\n",
    "# Imports\n",
    "from keras.layers import Embedding\n",
    "from numpy import unique\n",
    "\n",
    "# # Count the unique number of teams\n",
    "# n_teams = unique(games_season[]).shape[0]              \n",
    "\n",
    "# Create an embedding layer\n",
    "car_lookup = Embedding(input_dim=n_cols,\n",
    "                        output_dim=1,\n",
    "                        input_length=n_cols,\n",
    "                        name='Car-Strength')\n",
    "\n",
    "\n",
    "# Define team model\n",
    "# The team strength lookup has three components: an input, an embedding layer, and a flatten layer that creates the output.\n",
    "\n",
    "# If you wrap these three layers in a model with an input and output, you can re-use that stack of three layers at multiple places.\n",
    "\n",
    "# Note again that the weights for all three layers will be shared everywhere we use them.\n",
    "# Create a 1D input layer for the team ID (which will be an integer). Be sure to set the correct input shape!\n",
    "# Pass this input to the team strength lookup layer you created previously.\n",
    "# Flatten the output of the team strength lookup.\n",
    "# Create a model that uses the 1D input as input and flattened team strength as output.\n",
    "\n",
    "# Imports\n",
    "from keras.layers import Input, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "# Create an input layer for the team ID\n",
    "teamid_in = Input(shape=(n_cols,))\n",
    "\n",
    "# Lookup the input in the team strength embedding layer\n",
    "strength_lookup = car_lookup(teamid_in)\n",
    "\n",
    "# Flatten the output\n",
    "strength_lookup_flat = Flatten()(strength_lookup)\n",
    "\n",
    "# Combine the operations into a single, re-usable model\n",
    "team_strength_model = Model(teamid_in, strength_lookup_flat, name='Car-Strength-Model')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# fitting\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=14,\n",
    "          batch_size=128,\n",
    "          validation_split=0.1,\n",
    "          verbose=True)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print('Evaloate:  ',model.evaluate(x_test, y_test, verbose=False))\n",
    "\n",
    "\n",
    "# Print the model's weights\n",
    "print(model.get_weights())\n",
    "\n",
    "# Print the column means of the training data\n",
    "print(x_train.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[2.],\n       [2.],\n       [2.],\n       ...,\n       [1.],\n       [2.],\n       [0.]])]...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-eda25e240aca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m           batch_size=16384)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[2.],\n       [2.],\n       [2.],\n       ...,\n       [1.],\n       [2.],\n       [0.]])]..."
     ]
    }
   ],
   "source": [
    "# # # Creating a Single model for classification and regression \n",
    "# # evaluation has 3 parameters: [loss used by the model(sum of the output losses),\n",
    "# #                               loss as the regression part of the model, \n",
    "# #                              loss for classification part of the model]\n",
    "\n",
    "# Classification and regression in one model\n",
    "# Now you will create a different kind of 2-output model. This time, you will predict the score difference, instead of both team's scores and then you will predict the probability that team 1 won the game. This is a pretty cool model: it is going to do both classification and regression!\n",
    "\n",
    "# In this model, turn off the bias, or intercept for each layer. Your inputs (seed difference and predicted score difference) have a mean of very close to zero, and your outputs both have means that are close to zero, so your model shouldn't need the bias term to fit the data well.\n",
    "\n",
    "\n",
    "# Create an input layer with 2 columns\n",
    "input_tensor = Input(shape=(2,))\n",
    "\n",
    "# Create the first output\n",
    "output_tensor_1 = Dense(1, activation='linear', use_bias=False)(input_tensor)\n",
    "\n",
    "# Create the second output (use the first output as input here)\n",
    "output_tensor_2 = Dense(1, activation='sigmoid', use_bias=False)(output_tensor_1)\n",
    "\n",
    "# Create a model with 2 outputs\n",
    "model = Model(input_tensor, [output_tensor_1, output_tensor_2])\n",
    "\n",
    "# Compile and fit the model\n",
    "# Now that you have a model with 2 outputs, compile it with 2 loss functions: mean absolute error (MAE) for 'score_diff' and binary cross-entropy (also known as logloss) for 'won'. Then fit the model with 'seed_diff' and 'pred' as inputs. For outputs, predict 'score_diff' and 'won'.\n",
    "\n",
    "# This model can use the scores of the games to make sure that close games (small score diff) have lower win probabilities than blowouts (large score diff).\n",
    "\n",
    "# The regression problem is easier than the classification problem because MAE punishes the model less for a loss due to random chance. For example, if score_diff is -1 and won is 0, that means team_1 had some bad luck and lost by a single free throw. The data for the easy problem helps the model find a solution to the hard problem.\n",
    "\n",
    "# Import the Adam optimizer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Compile the model with 2 losses and the Adam optimzer with a higher learning rate\n",
    "model.compile(loss=['mean_absolute_error', 'binary_crossentropy'], optimizer=Adam(lr=0.01))\n",
    "\n",
    "# Fit the model to the tournament training data, with 2 inputs and 2 outputs\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "          epochs=10,\n",
    "          verbose=True,\n",
    "          validation_split=0.1,\n",
    "          batch_size=16384)\n",
    "\n",
    "\n",
    "# Inspect the model (II)\n",
    "# Now you should take a look at the weights for this model. In particular, note the last weight of the model. This weight converts the predicted score difference to a predicted win probability. If you multiply the predicted score difference by the last weight of the model and then apply the sigmoid function, you get the win probability of the game.\n",
    "\n",
    "\n",
    "# Print the model weights\n",
    "print(model.get_weights())\n",
    "\n",
    "# Print the training data means\n",
    "print(x_train.mean())\n",
    "\n",
    "# Inspect the model (II)\n",
    "# Now you should take a look at the weights for this model. In particular, note the last weight of the model. This weight converts the predicted score difference to a predicted win probability. If you multiply the predicted score difference by the last weight of the model and then apply the sigmoid function, you get the win probability of the game.\n",
    "\n",
    "# Import the sigmoid function from scipy\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# Weight from the model\n",
    "weight = 0.14\n",
    "\n",
    "# Print the approximate win probability predicted close game\n",
    "print(sigmoid(1 * weight))\n",
    "\n",
    "# Print the approximate win probability predicted blowout game\n",
    "print(sigmoid(10 * weight))\n",
    "\n",
    "# Evaluate on new data with two metrics\n",
    "# Now that you've fit your model and inspected its weights to make sure they make sense, evaluate your model on the tournament test set to see how well it does on new data.\n",
    "\n",
    "# Note that in this case, Keras will return 3 numbers: the first number will be the sum of both the loss functions, and then the next 2 numbers will be the loss functions you used when defining the model.\n",
    "\n",
    "\n",
    "# Evaluate the model on new data\n",
    "print(model.evaluate(x_test,y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
